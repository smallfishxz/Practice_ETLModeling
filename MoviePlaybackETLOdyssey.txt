MoviePlaybackETLOdyssey
[edit]Problem Statement

Your mission is to build an ETL process to populate a dashboard that shows the top movies viewed for a streaming movie service.  We'll assume our streaming movie service is successful - we have millions of daily users.

Imagine we have a logging system in place that captures user playback events such as:

(userid, movieid, PLAYBACK_STARTED, movie_pos_in_secs, utc_timestamp) 
(userid, movieid, PLAYBACK_STOPPED, movie_pos_in_secs, utc_timestamp) 
Further, imagine our data infrastructure team has made our lives easier in Data Engineering by building for us an upstream table that pairs up playback start and stop events as playback segments:

playback_segments = (userid, movieid, pb_start_pos, pb_end_pos, pb_start_timestamp, pb_end_timestamp)

Each record represents one continuously played segment of a movie.  A user may view many segments of the same movie over the course of a single day, and may rewind or fast-forward between viewing.  So for a given (userid, movieid, date), there may be many possibly overlapping or disjoint playback_segments that start and/or end within that day.

During a weekly meeting, a Product Manager asks you to if could build a dashboard that shows the following:

By date - By movie category - Show the Top 10 most viewed movies (with rank numbers 1-10)"

How would you go about building an ETL pipeline to provide data for the dashboard?

[edit]Clarifying the Requirements

There are some details that need clarification before a pipeline and dashboard can be built.  The best candidates will discuss each of the following points:

How is "Most Viewed" defined?  What counts as a "view"? If a user watches the first 3 seconds of a 90 minute movie, does that count as a view?  What if a user finds a particular 5 min scene in a 90 min movie especially entertaining, and watches that one scene over and over, 20 times in the same day - does that count as a view?  Or does it count as 20 views?  What if we just sum up the lengths of all playback segments for each movie, and rank them based on that?  What we want to do here is steer the candidate towards defining the metric like this: "Most Viewed" is determined by ranking movies based on the number of unique users who viewed at least a threshold percentage - 50% or 90% or whatever the candidate suggests - of the total movie duration on the target date.  Since playback_segments may overlap, we'll need to somehow flatten them out so that we're counting the total number of uniquely watched seconds of the movie per user.
Where does Movie Category come from?  Many candidates will suggest creating a movie dimension table, ideally including overall_movie_duration_secs to enable the percentage viewed calculation to count unique viewers. They will add category as an attribute of the movie dimension.  Time to refine the dashboard requirements a bit!  Suggest that movies may belong to several categories, and may be ranked differently relative to each.  A recent release may be both a Comedy and a Drama, and may be ranked #1 in Comedy and #8 in Drama on a given date.  Ask the candidate to draw out a data model.  They'll likely create an association table to handle the many-to-many relationship between Movies and Categories.  
How are movie views attributed to a date, in cases where the user watched part of the movie today and part of the movie yesterday?  What if a playback_segment spans two dates - that is, midnight falls between pb_start_timestamp and pb_end_timestamp is after midnight?  This is one of those edge cases we often have to consider at FB.  A reasonable solution would be to just look at all the playback_segments where either pb_start_timestamp or pb_end_timestamp falls within the reporting date.  Most candidates will suggest filtering on  DATE(pb_start_timestamp).  Of course, to capture most of the playback segements that started yesterday, a batch job would have to wait a few hours past midnight to accumulate the segments that were in progress at midnight.  Or maybe playback_segments are automatically chopped up at midnight so they never span dates.  Usually candidates don't get this far into the details, which is just as well.
[edit]ETL Pipeline

There are a couple of steps that the candidate needs to work out:

We need a process that converts playback_segements (userid, movieid, pb_start_pos, pb_end_pos, pb_start_timestamp, pb_end_timestamp) into user_movie_views (userid, movieid, total_viewed_secs, date), where total_viewed_secs is counting uniquely viewed seconds of the movie, not double-counting the same portion of a movie viewed twice in a day.
We need a process that aggregates user_movie_views (userid, movieid, total_viewed_secs, date) to category_movie_rank (category_name, movie_name, rank_number, date).  This is what we will show on the dashboard.
[edit]MapReduce

For the first step of the ETL pipeline, we need some way of calculating unique movie seconds viewed by a user from a collection of disjoint  or partially/completely overlapping playback_segments.  Some candidates will attempt to write a SQL query to do this calculation, but that doesn't work.  Almost all candidates are familiar with map/reduce at least conceptually. The problem can be solved via a map/reduce:

Wait until our upstream source table has accumulated all of the playback_segments on that started (or maybe better, ended on) the target reporting date.
Filter the source data to include only the records that fall within our target date.  At this point, we're finished using the pb_start_timestamp and pb_end_timestamp fields.
The mapper stage will distribute the data by (userid, movieid) and sort the data by (userid, movieid, pb_start_pos)
All the playback_segments for a given (userid, movieid) will be sent to the same reducer and will be sorted in order of increasing pb_start_pos.  
Write a reducer program that reduces for a (userid, movied)  all of the sorted (pb_start_pos, pb_end_pos) tuples, and emits a single value total_viewed_secs. Here, the candidate should be able to write some Python or pseudo-code to implement a reducer algorithm.  Reduce a list of tuples (these are intervals, really) [(pb_start_pos_0, pb_end_pos_0), (pb_start_pos_1, pb_end_pos_1), ...., (pb_start_pos_N, pb_end_pos_N)] to a single value total_viewed_secs The output of our map-reduce is loaded into a date partition in user_movie_views = (userid, movieid, total_viewed_secs, date)

*** Thoughts for the reduce job: 
total = 0
last_start = 0
last_end = 0
For item in list:
  if item[0] >= last_start AND item[1] > last_end:
    total = total + (item[1] - last_end)
    last_start = item[0]
    last_end = item[1]

[edit]Summary SQL Query

Our MapReduce job has transformed the Big Data in playback_segments, and loaded Still Pretty Big Data at the (userid, movieid, date) level - assuming our streaming movie service is successful and has millions of daily users.  Now we need to further transform this data to load a summary table, along the lines of category_movie_rank = (category, movie_name, rank, date). This could be done by writing a Hive, Presto, Vertica, or some other sort of SQL query that will run in a scalable way against a large data set.  Things the candidate needs to take care of:

Compare (userid, movieid, total_viewed_secs, date) with overall_movie_duration_secs from our Movie dimension.  Count as unique viewers all of the records where total_viewed_secs >= threshold_pct * dim_movie.overall_movie_duration_secs.  Earlier, the candidate defined threshold_pct to be 50% or 90% or some such thing.
Join through our movie_category_assoc table, and rank movies *within each* category by unique viewers. Sometimes candidates get tripped up by ranking within category - they calculate the relative ranking of movies overall, and then join to category.  This approach will not correctly handle a Comedy/Drama that should be ranked #1 in Comedy and #8 in Drama.
[edit]Bonus Topics

There's rarely time to discuss additional ETL topics given how much ground this question needs to cover in 40min (leaving 5 min for Q&A).   For the rare candidate who finishes with a few minutes to spare, data quality assurance is a good topic to ask about - what changes would you make to your ETL process to handle unexpected issues in upstream data, to catch empty or double-loaded data sets before publication to the dashboard, and so on.
